import streamlit as st
import re
from collections import Counter
from PIL import Image
import pytesseract
import pdfplumber
from pdf2image import convert_from_bytes
import requests
from deep_translator import GoogleTranslator
from wordfreq import zipf_frequency

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
st.set_page_config(page_title="DE Tutor Pro", layout="wide")
st.title("üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π B2: –£–º–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å + –°–∏–Ω–æ–Ω–∏–º—ã")

STOP_WORDS = {
    "der", "die", "das", "und", "ist", "in", "zu", "den", "dem", "des", 
    "mit", "auf", "f√ºr", "von", "ein", "eine", "einen", "sich", "aus",
    "dass", "nicht", "war", "aber", "man", "bei", "wie", "wir", "oder",
    "kann", "sind", "werden", "wird", "auch", "noch", "nur", "vor", "nach",
    "√ºber", "wenn", "zum", "zur", "habe", "hat", "durch", "unter", "diese",
    "dieser", "ihre", "seine", "meine", "vom", "am", "im", "um", "als",
    "es", "sie", "er", "du", "ich", "mich", "mir", "dir", "uns", "ihnen",
    "diesen", "demnach", "dabei", "damit", "daf√ºr",
    "telc", "deutsch", "pr√ºfung", "test", "seite", "page", "express", "hueber",
    "aufgabe", "l√∂sung", "antwortbogen", "teil", "kapitel", "√ºbung", "verlag",
    "auflage", "gmbh", "druck", "isbn", "m√ºnchen", "klett", "cornelsen",
    "minuten", "punkte", "lesen", "h√∂ren", "schreiben", "sprechen",
    "text", "texte", "√ºberschrift", "√ºberschriften", "modelltest",
    "tipps", "tricks", "informationen", "antworten", "ankreuzen", "markieren",
    "richtig", "falsch", "insgesamt", "zeit", "beispiel", "nummer", "email"
}

# --- 2. –§—É–Ω–∫—Ü–∏–∏ ---

@st.cache_data
def estimate_level(word):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å (A1-C2)"""
    try:
        freq = zipf_frequency(word, 'de')
        if freq == 0: return "‚Äî"
        if freq > 5.5: return "A1"
        if freq > 4.5: return "A2"
        if freq > 3.8: return "B1" # –ß—É—Ç—å —Å–Ω–∏–∑–∏–ª –ø–æ—Ä–æ–≥ –¥–ª—è B1
        if freq > 3.0: return "B2"
        return "C1"
    except:
        return "?"

@st.cache_data
def get_translation(word):
    try:
        return GoogleTranslator(source='de', target='ru').translate(word)
    except:
        return "-"

@st.cache_data
def get_synonyms(word):
    """
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤.
    –ü—Ä–æ–±—É–µ—Ç —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å–ª–æ–≤–∞ (—É–±–∏—Ä–∞–µ—Ç –æ–∫–æ–Ω—á–∞–Ω–∏—è), –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ—Ç –æ—Ç–≤–µ—Ç.
    """
    
    def fetch_api(query):
        # –ó–∞–ø—Ä–æ—Å –∫ OpenThesaurus
        url = f"https://www.openthesaurus.de/synonyme/search?q={query}&format=json"
        try:
            r = requests.get(url, timeout=1)
            if r.status_code == 200:
                data = r.json()
                found = []
                for synset in data.get('synsets', []):
                    for term in synset.get('terms', []):
                        t = term.get('term')
                        # –ß–∏—Å—Ç–∏–º –æ—Ç –º—É—Å–æ—Ä–∞ (—É–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ –∏ —Ñ—Ä–∞–∑—ã)
                        t_clean = re.sub(r"\(.*?\)", "", t).strip()
                        if t_clean.lower() != query.lower() and len(t_clean.split()) < 3:
                            found.append(t_clean)
                return list(dict.fromkeys(found))
        except:
            return []
        return []

    # 1. –ü—Ä–æ–±—É–µ–º —Å–ª–æ–≤–æ –∫–∞–∫ –µ—Å—Ç—å
    syns = fetch_api(word)
    
    # 2. –ï—Å–ª–∏ –ø—É—Å—Ç–æ, –ø—Ä–æ–±—É–µ–º –æ—Ç—Ä–µ–∑–∞—Ç—å –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø—Ä–µ–≤—Ä–∞—â–∞–µ–º Plural –≤ Singular)
    if not syns and len(word) > 4:
        # Mahlzeiten -> Mahlzeit
        if word.endswith("en"): syns = fetch_api(word[:-2])
        # Autos -> Auto
        elif word.endswith("s"): syns = fetch_api(word[:-1])
        # Schule -> Schul (–∏–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç)
        elif word.endswith("e"): syns = fetch_api(word[:-1])
        # Lehrern -> Lehrer
        elif word.endswith("n"): syns = fetch_api(word[:-1])

    if syns:
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-4 —Å–∏–Ω–æ–Ω–∏–º–∞
        return ", ".join(syns[:4])
    
    return "‚Äî" # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏

def find_context(text, word):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sent in sentences:
        if re.search(r'\b' + re.escape(word) + r'\b', sent, re.IGNORECASE):
            clean = sent.replace("\n", " ").strip()
            return clean[:120] + "..." if len(clean) > 120 else clean
    return "‚Äî"

def extract_text(file_bytes, file_type, start, limit):
    text = ""
    start_idx = start - 1
    
    # PDF —Ç–µ–∫—Å—Ç
    if file_type == "application/pdf":
        try:
            with pdfplumber.open(file_bytes) as pdf:
                if start_idx < len(pdf.pages):
                    pages = pdf.pages[start_idx : start_idx + limit]
                    for p in pages:
                        t = p.extract_text()
                        if t: text += t + "\n"
        except: pass

    # OCR
    if len(text) < 50:
        if file_type == "application/pdf":
            st.info(f"üîé –í–∫–ª—é—á–∞—é OCR –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü {start}-{start+limit-1}...")
            try:
                file_bytes.seek(0)
                images = convert_from_bytes(file_bytes.read(), first_page=start, last_page=start+limit-1)
                for img in images:
                    text += pytesseract.image_to_string(img, lang='deu') + "\n"
            except: pass
        else:
            img = Image.open(file_bytes)
            text = pytesseract.image_to_string(img, lang='deu')
            
    return text

def process_text(text, min_len):
    clean_text = re.sub(r'[^a-zA-Z√§√∂√º√Ñ√ñ√ú√ü\s]', '', text)
    words = clean_text.split()
    filtered = []
    for w in words:
        w_clean = w.strip()
        if len(w_clean) >= min_len and w_clean.lower() not in STOP_WORDS and not w_clean.isdigit():
            filtered.append(w_clean)
    return Counter(filtered).most_common()

# --- 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    # –¢–≤–æ–π —Ñ–∞–π–ª –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ä–µ–∫–ª–∞–º—ã, —Ç–µ–∫—Å—Ç—ã –∏–¥—É—Ç –ø–æ–∑–∂–µ.
    # –î–ª—è —Ç–µ—Å—Ç–∞ –ø—Ä–æ –µ–¥—É —Å—Ç–∞–≤—å 15. –î–ª—è —Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ Resilienz —Å—Ç–∞–≤—å 54.
    start_page = st.number_input("–ù–∞—á–∞—Ç—å —Å–æ —Å—Ç—Ä.", 1, 200, 54, help="–°—Ç—Ä–∞–Ω–∏—Ü–∞ —É—á–µ–±–Ω–∏–∫–∞")
    pages_to_read = st.slider("–ß–∏—Ç–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü", 1, 3, 1)
    max_words = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤", 10, 40, 15)

st.write("### üá©üá™ B2 Trainer: –°–ª–æ–≤–∞ + –ü–µ—Ä–µ–≤–æ–¥ + –°–∏–Ω–æ–Ω–∏–º—ã")

uploaded_file = st.file_uploader("–§–∞–π–ª", type=['pdf', 'jpg', 'png'])

if uploaded_file and st.button("üöÄ –ù–∞—á–∞—Ç—å —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É"):
    with st.spinner("–†–∞–±–æ—Ç–∞–µ–º: —á–∏—Ç–∞—é, –ø–µ—Ä–µ–≤–æ–∂—É, –∏—â—É —Å–∏–Ω–æ–Ω–∏–º—ã –≤ —Å–ª–æ–≤–∞—Ä–µ..."):
        full_text = extract_text(uploaded_file, uploaded_file.type, start_page, pages_to_read)
        
        if len(full_text) < 10:
            st.error("–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –í–æ–∑–º–æ–∂–Ω–æ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞.")
        else:
            # –ê–Ω–∞–ª–∏–∑
            freq_list = process_text(full_text, 4) # –º–∏–Ω –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞ 4 –±—É–∫–≤—ã
            top_words = freq_list[:max_words]
            
            table_data = []
            bar = st.progress(0)
            
            for i, (word, count) in enumerate(top_words):
                lvl = estimate_level(word)
                trans = get_translation(word)
                syns = get_synonyms(word) # –¢—É—Ç —Ç–µ–ø–µ—Ä—å —Ä–∞–±–æ—Ç–∞–µ—Ç "–ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫"
                ctx = find_context(full_text, word)
                
                table_data.append({
                    "–£—Ä–æ–≤–µ–Ω—å": lvl,
                    "–°–ª–æ–≤–æ": word,
                    "–ü–µ—Ä–µ–≤–æ–¥ (RU)": trans,
                    "–°–∏–Ω–æ–Ω–∏–º—ã (B2)": syns,
                    "–ö–æ–Ω—Ç–µ–∫—Å—Ç": ctx,
                    "–í—ã—É—á–∏—Ç—å": False
                })
                bar.progress((i+1)/len(top_words))
            
            st.success(f"–ì–æ—Ç–æ–≤–æ! –°–ª–æ–≤–∞—Ä—å –æ–±–Ω–æ–≤–ª–µ–Ω.")
            
            st.data_editor(
                table_data,
                column_config={
                    "–£—Ä–æ–≤–µ–Ω—å": st.column_config.TextColumn("Lvl", width="small"),
                    "–°–∏–Ω–æ–Ω–∏–º—ã (B2)": st.column_config.TextColumn("–°–∏–Ω–æ–Ω–∏–º—ã (B2)", width="large", help="–°–ª–æ–≤–∞ –¥–ª—è –∑–∞–º–µ–Ω—ã –Ω–∞ —ç–∫–∑–∞–º–µ–Ω–µ"),
                    "–í—ã—É—á–∏—Ç—å": st.column_config.CheckboxColumn("‚úÖ")
                },
                height=800,
                hide_index=True
            )
