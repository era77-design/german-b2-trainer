import streamlit as st
import re
import pandas as pd # –ù–æ–≤–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è Excel/CSV
from collections import Counter
from PIL import Image
import pytesseract
import pdfplumber
from pdf2image import convert_from_bytes
import requests
from deep_translator import GoogleTranslator
from wordfreq import zipf_frequency
import random

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
st.set_page_config(page_title="DE Tutor B2", layout="wide")

STOP_WORDS = {
    "der", "die", "das", "und", "ist", "in", "zu", "den", "dem", "des", 
    "mit", "auf", "f√ºr", "von", "ein", "eine", "einen", "sich", "aus",
    "dass", "nicht", "war", "aber", "man", "bei", "wie", "wir", "oder",
    "kann", "sind", "werden", "wird", "auch", "noch", "nur", "vor", "nach",
    "√ºber", "wenn", "zum", "zur", "habe", "hat", "durch", "unter", "diese",
    "telc", "deutsch", "pr√ºfung", "test", "seite", "page", "express", "hueber",
    "aufgabe", "l√∂sung", "antwortbogen", "teil", "kapitel", "√ºbung", "verlag",
    "auflage", "gmbh", "druck", "isbn", "m√ºnchen", "klett", "cornelsen",
    "minuten", "punkte", "lesen", "h√∂ren", "schreiben", "sprechen",
    "text", "texte", "√ºberschrift", "√ºberschriften", "modelltest",
    "tipps", "tricks", "informationen", "antworten", "ankreuzen", "markieren",
    "richtig", "falsch", "insgesamt", "zeit", "beispiel", "nummer", "email", "euro"
}

# --- 2. –§—É–Ω–∫—Ü–∏–∏ ---

@st.cache_data
def estimate_level(word):
    try:
        freq = zipf_frequency(word, 'de')
        if freq == 0: return "‚Äî"
        if freq > 5.5: return "A1"
        if freq > 4.5: return "A2"
        if freq > 3.8: return "B1"
        if freq > 2.8: return "B2" # B2 - —ç—Ç–æ —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞
        return "C1"
    except: return "?"

@st.cache_data
def get_translation(word):
    try: return GoogleTranslator(source='de', target='ru').translate(word)
    except: return "-"

@st.cache_data
def get_synonyms(word):
    def fetch_api(query):
        url = f"https://www.openthesaurus.de/synonyme/search?q={query}&format=json"
        try:
            r = requests.get(url, timeout=1)
            if r.status_code == 200:
                data = r.json()
                found = []
                for synset in data.get('synsets', []):
                    for term in synset.get('terms', []):
                        t = re.sub(r"\(.*?\)", "", term.get('term')).strip()
                        if t.lower() != query.lower() and len(t.split()) < 3:
                            found.append(t)
                return list(dict.fromkeys(found))
        except: return []
        return []

    syns = fetch_api(word)
    if not syns and len(word) > 4:
        if word.endswith("en"): syns = fetch_api(word[:-2])
        elif word.endswith("s") or word.endswith("n"): syns = fetch_api(word[:-1])
    
    return ", ".join(syns[:4]) if syns else "‚Äî"

def extract_text(file_bytes, file_type, start, limit):
    text = ""
    start_idx = start - 1
    if file_type == "application/pdf":
        try:
            with pdfplumber.open(file_bytes) as pdf:
                if start_idx < len(pdf.pages):
                    pages = pdf.pages[start_idx : start_idx + limit]
                    for p in pages:
                        t = p.extract_text()
                        if t: text += t + "\n"
        except: pass

    if len(text) < 50 and file_type == "application/pdf":
        st.info(f"üîé OCR —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥ —Å—Ç—Ä. {start}-{start+limit-1}...")
        try:
            file_bytes.seek(0)
            images = convert_from_bytes(file_bytes.read(), first_page=start, last_page=start+limit-1)
            for img in images:
                text += pytesseract.image_to_string(img, lang='deu') + "\n"
        except: pass
    elif file_type != "application/pdf":
        img = Image.open(file_bytes)
        text = pytesseract.image_to_string(img, lang='deu')
    return text

def process_text(text, min_len):
    clean_text = re.sub(r'[^a-zA-Z√§√∂√º√Ñ√ñ√ú√ü\s]', '', text)
    words = clean_text.split()
    filtered = []
    for w in words:
        if len(w) >= min_len and w.lower() not in STOP_WORDS and not w.isdigit():
            filtered.append(w)
    return Counter(filtered).most_common()

def find_context(text, word):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sent in sentences:
        if re.search(r'\b' + re.escape(word) + r'\b', sent, re.IGNORECASE):
            return sent.replace("\n", " ").strip()[:150]
    return "‚Äî"

# --- 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

st.title("üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π B2: –ê–Ω–∞–ª–∏–∑ + –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞")

# –í–∫–ª–∞–¥–∫–∏: –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ | –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ (–ö–≤–∏–∑)
tab1, tab2 = st.tabs(["üìÇ –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å", "üéì –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ (–ö–≤–∏–∑)"])

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö
if 'vocab_df' not in st.session_state:
    st.session_state.vocab_df = pd.DataFrame()

with tab1:
    with st.sidebar:
        st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
        start_page = st.number_input("–°—Ç—Ä–∞–Ω–∏—Ü–∞", 1, 300, 54)
        pages_limit = st.slider("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü", 1, 3, 1)
        max_words = st.slider("–°–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä—å", 10, 50, 15)

    uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ PDF", type=['pdf', 'jpg'])

    if uploaded_file and st.button("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
        with st.spinner("–°–æ–∑–¥–∞—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö..."):
            full_text = extract_text(uploaded_file, uploaded_file.type, start_page, pages_limit)
            
            if len(full_text) > 10:
                freq_list = process_text(full_text, 4)
                top_words = freq_list[:max_words]
                
                data = []
                prog = st.progress(0)
                for i, (word, count) in enumerate(top_words):
                    lvl = estimate_level(word)
                    trans = get_translation(word)
                    syns = get_synonyms(word)
                    ctx = find_context(full_text, word)
                    
                    data.append({
                        "–°–ª–æ–≤–æ": word,
                        "–ü–µ—Ä–µ–≤–æ–¥": trans,
                        "–°–∏–Ω–æ–Ω–∏–º—ã": syns,
                        "–ö–æ–Ω—Ç–µ–∫—Å—Ç": ctx,
                        "–£—Ä–æ–≤–µ–Ω—å": lvl
                    })
                    prog.progress((i+1)/len(top_words))
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏
                st.session_state.vocab_df = pd.DataFrame(data)
                st.success(f"–ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ {len(data)} —Å–ª–æ–≤.")
            else:
                st.error("–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω.")

    # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏ –∫–Ω–æ–ø–∫—É —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    if not st.session_state.vocab_df.empty:
        df = st.session_state.vocab_df
        st.data_editor(df, hide_index=True)
        
        # –ö–ù–û–ü–ö–ê –°–ö–ê–ß–ò–í–ê–ù–ò–Ø (–≠–∫—Å–ø–æ—Ä—Ç –≤ CSV)
        csv = df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å (CSV –¥–ª—è Excel/Anki)",
            data=csv,
            file_name='mein_wortschatz_b2.csv',
            mime='text/csv',
        )

with tab2:
    st.header("–ü—Ä–æ–≤–µ—Ä—å —Å–µ–±—è")
    
    if st.session_state.vocab_df.empty:
        st.warning("–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∞–π–ª –≤–æ –≤–∫–ª–∞–¥–∫–µ '–°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å'!")
    else:
        # –õ–æ–≥–∏–∫–∞ –∫–≤–∏–∑–∞
        if 'current_word' not in st.session_state:
            st.session_state.current_word = st.session_state.vocab_df.sample(1).iloc[0]
            st.session_state.show_answer = False

        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üá©üá™ –°–ª–æ–≤–æ:")
            st.markdown(f"# {st.session_state.current_word['–°–ª–æ–≤–æ']}")
            
            st.info(f"üí° –ö–æ–Ω—Ç–µ–∫—Å—Ç: *{st.session_state.current_word['–ö–æ–Ω—Ç–µ–∫—Å—Ç']}*")
            
            if st.button("–ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–µ–≤–æ–¥"):
                st.session_state.show_answer = True

        with col2:
            if st.session_state.show_answer:
                st.subheader("üá∑üá∫ –ü–µ—Ä–µ–≤–æ–¥:")
                st.success(f"**{st.session_state.current_word['–ü–µ—Ä–µ–≤–æ–¥']}**")
                
                st.subheader("üîó –°–∏–Ω–æ–Ω–∏–º—ã (B2):")
                st.warning(st.session_state.current_word['–°–∏–Ω–æ–Ω–∏–º—ã'])
                
                if st.button("‚û° –°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ"):
                    st.session_state.current_word = st.session_state.vocab_df.sample(1).iloc[0]
                    st.session_state.show_answer = False
                    st.rerun()
