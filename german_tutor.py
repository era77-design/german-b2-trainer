import streamlit as st
import re
import pandas as pd
import time
from collections import Counter
from PIL import Image
import pytesseract
import pdfplumber
from pdf2image import convert_from_bytes
import requests
from deep_translator import GoogleTranslator
from wordfreq import zipf_frequency
import gc # –°–±–æ—Ä—â–∏–∫ –º—É—Å–æ—Ä–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---
st.set_page_config(page_title="DE B2 Master", layout="wide")

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞)
STOP_WORDS = {
    "der", "die", "das", "und", "ist", "in", "zu", "den", "dem", "des", 
    "mit", "auf", "f√ºr", "von", "ein", "eine", "einen", "sich", "aus",
    "dass", "nicht", "war", "aber", "man", "bei", "wie", "wir", "oder",
    "kann", "sind", "werden", "wird", "auch", "noch", "nur", "vor", "nach",
    "√ºber", "wenn", "zum", "zur", "habe", "hat", "durch", "unter", "diese",
    "telc", "deutsch", "pr√ºfung", "test", "seite", "page", "express", "hueber",
    "aufgabe", "l√∂sung", "antwortbogen", "teil", "kapitel", "√ºbung", "verlag",
    "auflage", "gmbh", "druck", "isbn", "m√ºnchen", "klett", "cornelsen",
    "minuten", "punkte", "lesen", "h√∂ren", "schreiben", "sprechen",
    "text", "texte", "√ºberschrift", "√ºberschriften", "modelltest",
    "tipps", "tricks", "informationen", "antworten", "ankreuzen", "markieren",
    "richtig", "falsch", "insgesamt", "zeit", "beispiel", "nummer", "email", "euro"
}

# --- 2. –§—É–Ω–∫—Ü–∏–∏ –ª–æ–≥–∏–∫–∏ ---

@st.cache_data
def estimate_level(word):
    """–û—Ü–µ–Ω–∫–∞ —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (A1-C2)"""
    try:
        freq = zipf_frequency(word, 'de')
        if freq == 0: return "‚Äî"
        if freq > 5.5: return "A1"
        if freq > 4.5: return "A2"
        if freq > 3.8: return "B1"
        if freq > 2.8: return "B2"
        return "C1"
    except: return "?"

@st.cache_data
def get_translation(word):
    """–ü–µ—Ä–µ–≤–æ–¥ Google"""
    try: return GoogleTranslator(source='de', target='ru').translate(word)
    except: return "-"

@st.cache_data
def get_synonyms(word):
    """
    –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤.
    –ï—Å–ª–∏ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç —Å–ª–æ–≤–æ, –ø—ã—Ç–∞–µ—Ç—Å—è —É–±—Ä–∞—Ç—å –æ–∫–æ–Ω—á–∞–Ω–∏—è (Plural -> Singular).
    """
    def fetch_api(query):
        url = f"https://www.openthesaurus.de/synonyme/search?q={query}&format=json"
        try:
            r = requests.get(url, timeout=1)
            if r.status_code == 200:
                data = r.json()
                found = []
                for synset in data.get('synsets', []):
                    for term in synset.get('terms', []):
                        # –ß–∏—Å—Ç–∏–º –æ—Ç —Å–∫–æ–±–æ–∫ (ugs.)
                        t = re.sub(r"\(.*?\)", "", term.get('term')).strip()
                        # –§–∏–ª—å—Ç—Ä: –Ω–µ —Å–∞–º–æ —Å–ª–æ–≤–æ –∏ –Ω–µ —Ñ—Ä–∞–∑–∞ –∏–∑ 3 —Å–ª–æ–≤
                        if t.lower() != query.lower() and len(t.split()) < 3:
                            found.append(t)
                return list(dict.fromkeys(found))
        except: return []
        return []

    syns = fetch_api(word)
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–∞—Ç—å –æ–∫–æ–Ω—á–∞–Ω–∏—è
    if not syns and len(word) > 4:
        if word.endswith("en"): syns = fetch_api(word[:-2])
        elif word.endswith("s") or word.endswith("n"): syns = fetch_api(word[:-1])
    
    return ", ".join(syns[:4]) if syns else "‚Äî"

def process_text_chunk(text):
    """–ß–∏—Å—Ç–∏—Ç –∫—É—Å–æ–∫ —Ç–µ–∫—Å—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤"""
    clean_text = re.sub(r'[^a-zA-Z√§√∂√º√Ñ√ñ√ú√ü\s]', '', text)
    words = clean_text.split()
    filtered = []
    for w in words:
        # –§–∏–ª—å—Ç—Ä: –º–∏–Ω –¥–ª–∏–Ω–∞ 4, –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ, –Ω–µ —á–∏—Å–ª–æ
        if len(w) >= 4 and w.lower() not in STOP_WORDS and not w.isdigit():
            filtered.append(w)
    return filtered

def find_context(text, word):
    """–ò—â–µ—Ç –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞"""
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sent in sentences:
        if re.search(r'\b' + re.escape(word) + r'\b', sent, re.IGNORECASE):
            return sent.replace("\n", " ").strip()[:120]
    return "‚Äî"

def process_pdf_full(file_obj, start_p, num_pages):
    """
    –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—å –ø–∞–º—è—Ç—å.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤ –∏ —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç (–¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞).
    """
    all_words = []
    full_context_text = ""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    file_bytes = file_obj.read()
    
    # –¶–∏–∫–ª –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º
    for i in range(num_pages):
        current_page_idx = start_p - 1 + i
        status_text.text(f"‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {start_p + i}...")
        
        page_text = ""
        
        # 1. –ü—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å —Ç–µ–∫—Å—Ç (–±—ã—Å—Ç—Ä–æ)
        try:
            with pdfplumber.open(file_obj) as pdf:
                if current_page_idx < len(pdf.pages):
                    page_text = pdf.pages[current_page_idx].extract_text()
        except: pass

        # 2. –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–µ—Ç -> OCR (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –Ω–∞–¥–µ–∂–Ω–æ)
        if not page_text or len(page_text) < 50:
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –¢–û–õ–¨–ö–û –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∫–∞—Ä—Ç–∏–Ω–∫—É
                images = convert_from_bytes(
                    file_bytes, 
                    first_page=current_page_idx+1, 
                    last_page=current_page_idx+1
                )
                if images:
                    page_text = pytesseract.image_to_string(images[0], lang='deu')
                    del images # –£–¥–∞–ª—è–µ–º –∫–∞—Ä—Ç–∏–Ω–∫—É –∏–∑ –ø–∞–º—è—Ç–∏
                    gc.collect() # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ OCR –Ω–∞ —Å—Ç—Ä {current_page_idx+1}: {e}")

        if page_text:
            # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞
            words_in_page = process_text_chunk(page_text)
            all_words.extend(words_in_page)
            full_context_text += page_text + "\n"
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        progress_bar.progress((i + 1) / num_pages)

    return all_words, full_context_text

# --- 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

st.title("üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π B2: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –ë–∞–∑—ã –¥–ª—è Quizlet")
st.markdown("–ó–∞–≥—Ä—É–∑–∏ PDF, –≤—ã–±–µ—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏ —è —Å–æ–∑–¥–∞–º —Ñ–∞–π–ª –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ —Å–ª–æ–≤.")

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–∞–Ω–Ω—ã—Ö
if 'vocab_df' not in st.session_state:
    st.session_state.vocab_df = pd.DataFrame()

with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
    st.info("‚ö†Ô∏è –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ–π –∫–Ω–∏–≥–∏ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 5-10 –º–∏–Ω—É—Ç. –î–ª—è —Ç–µ—Å—Ç–∞ –≤—ã–±–µ—Ä–∏ 2-3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
    
    start_page = st.number_input("–° –∫–∞–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞—á–∞—Ç—å?", 1, 500, 54)
    pages_to_scan = st.number_input("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å?", 1, 50, 2)
    max_vocab_size = st.slider("–ú–∞–∫—Å–∏–º—É–º —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ", 10, 100, 20)

uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ —É—á–µ–±–Ω–∏–∫ (PDF)", type=['pdf'])

if uploaded_file and st.button("üöÄ –ù–∞—á–∞—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"):
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ
    st.session_state.vocab_df = pd.DataFrame()
    
    # 1. –°–±–æ—Ä —Å–ª–æ–≤
    with st.spinner("–ß–∏—Ç–∞—é –∫–Ω–∏–≥—É..."):
        # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–∞–π–ª –≤ —Ñ—É–Ω–∫—Ü–∏—é (–≤–∞–∂–Ω–æ: seek(0) –≤–Ω—É—Ç—Ä–∏ pdfplumber –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å—Å—è)
        uploaded_file.seek(0)
        raw_words, full_text = process_pdf_full(uploaded_file, start_page, pages_to_scan)
        
    if not raw_words:
        st.error("–°–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
    else:
        # 2. –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ (—Ç–æ–ø —Å–ª–æ–≤)
        st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(raw_words)} —Å–ª–æ–≤. –û—Ç–±–∏—Ä–∞—é —Ç–æ–ø-{max_vocab_size} —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö...")
        top_words_tuples = Counter(raw_words).most_common(max_vocab_size)
        
        # 3. –ü–µ—Ä–µ–≤–æ–¥ –∏ –°–∏–Ω–æ–Ω–∏–º—ã (—Å–∞–º–∞—è –¥–æ–ª–≥–∞—è —á–∞—Å—Ç—å)
        data = []
        vocab_bar = st.progress(0)
        
        for idx, (word, count) in enumerate(top_words_tuples):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —Å–ª–æ–≤–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –∏–ª–∏ –º—É—Å–æ—Ä
            if len(word) < 3: continue
                
            lvl = estimate_level(word)
            trans = get_translation(word)
            syns = get_synonyms(word)
            ctx = find_context(full_text, word)
            
            data.append({
                "–°–ª–æ–≤–æ (Term)": word,
                "–ü–µ—Ä–µ–≤–æ–¥ (Definition)": trans,
                "–°–∏–Ω–æ–Ω–∏–º—ã": syns,
                "–£—Ä–æ–≤–µ–Ω—å": lvl,
                "–ö–æ–Ω—Ç–µ–∫—Å—Ç": ctx
            })
            vocab_bar.progress((idx + 1) / len(top_words_tuples))
            
        st.session_state.vocab_df = pd.DataFrame(data)
        st.success("–ì–æ—Ç–æ–≤–æ! –°–ª–æ–≤–∞—Ä—å —Å–æ–∑–¥–∞–Ω.")

# --- 4. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –≠–∫—Å–ø–æ—Ä—Ç ---

if not st.session_state.vocab_df.empty:
    df = st.session_state.vocab_df
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
    st.subheader("–¢–≤–æ–π –°–ª–æ–≤–∞—Ä—å")
    st.data_editor(df, hide_index=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è Excel/GitHub (CSV —Å —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π)
        csv_excel = df.to_csv(index=False, sep=';').encode('utf-8-sig')
        st.download_button(
            label="üíæ –°–∫–∞—á–∞—Ç—å CSV (–¥–ª—è Excel/GitHub)",
            data=csv_excel,
            file_name=f'german_b2_pages_{start_page}_{start_page+pages_to_scan}.csv',
            mime='text/csv',
        )
        
    with col2:
        # –≠–∫—Å–ø–æ—Ä—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Quizlet
        # –§–æ—Ä–º–∞—Ç: –°–ª–æ–≤–æ [TAB] –ü–µ—Ä–µ–≤–æ–¥ + –°–∏–Ω–æ–Ω–∏–º—ã
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏ —Å–∏–Ω–æ–Ω–∏–º—ã –≤ –æ–¥–Ω–æ –ø–æ–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        quizlet_data = ""
        for index, row in df.iterrows():
            term = row['–°–ª–æ–≤–æ (Term)']
            # –í –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∏—à–µ–º: –ü–µ—Ä–µ–≤–æ–¥ (–°–∏–Ω–æ–Ω–∏–º—ã: ...) [–ö–æ–Ω—Ç–µ–∫—Å—Ç: ...]
            definition = f"{row['–ü–µ—Ä–µ–≤–æ–¥ (Definition)']} (Syn: {row['–°–∏–Ω–æ–Ω–∏–º—ã']})"
            quizlet_data += f"{term}\t{definition}\n"
            
        st.download_button(
            label="ü¶â –°–∫–∞—á–∞—Ç—å –¥–ª—è Quizlet (Copy-Paste)",
            data=quizlet_data.encode('utf-8'),
            file_name='quizlet_import.txt',
            mime='text/plain',
            help="–ó–∞–≥—Ä—É–∑–∏ —ç—Ç–æ—Ç —Ñ–∞–π–ª –≤ Quizlet —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é 'Import from Word, Excel, Google Docs'"
        )

    st.info("üí° **–ö–∞–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ Quizlet:**\n1. –û—Ç–∫—Ä–æ–π Quizlet -> 'Create Set'.\n2. –ù–∞–∂–º–∏ '+ Import from Word, Excel, Google Docs'.\n3. –û—Ç–∫—Ä–æ–π —Å–∫–∞—á–∞–Ω–Ω—ã–π txt-—Ñ–∞–π–ª, —Å–∫–æ–ø–∏—Ä—É–π –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏ –≤—Å—Ç–∞–≤—å —Ç—É–¥–∞.")
