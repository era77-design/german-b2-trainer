import streamlit as st
import re
import pandas as pd
import time
from collections import Counter
from PIL import Image
import pytesseract
import pdfplumber
from pdf2image import convert_from_bytes
import requests
from deep_translator import GoogleTranslator
from wordfreq import zipf_frequency
import gc 

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
st.set_page_config(page_title="DE B2 Master", layout="wide")

# –°—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞ + –∏–º–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑ –∫–Ω–∏–≥–∏)
STOP_WORDS = {
    "der", "die", "das", "und", "ist", "in", "zu", "den", "dem", "des", 
    "mit", "auf", "f√ºr", "von", "ein", "eine", "einen", "sich", "aus",
    "dass", "nicht", "war", "aber", "man", "bei", "wie", "wir", "oder",
    "kann", "sind", "werden", "wird", "auch", "noch", "nur", "vor", "nach",
    "√ºber", "wenn", "zum", "zur", "habe", "hat", "durch", "unter", "diese",
    "telc", "deutsch", "pr√ºfung", "test", "seite", "page", "express", "hueber",
    "aufgabe", "l√∂sung", "antwortbogen", "teil", "kapitel", "√ºbung", "verlag",
    "auflage", "gmbh", "druck", "isbn", "m√ºnchen", "klett", "cornelsen",
    "minuten", "punkte", "lesen", "h√∂ren", "schreiben", "sprechen",
    "text", "texte", "√ºberschrift", "√ºberschriften", "modelltest",
    "tipps", "tricks", "informationen", "antworten", "ankreuzen", "markieren",
    "richtig", "falsch", "insgesamt", "zeit", "beispiel", "nummer", "email", 
    "euro", "dagmar", "giersberg", "track", "transkriptionen"
}

# --- 2. –§—É–Ω–∫—Ü–∏–∏ ---

@st.cache_data
def estimate_level(word):
    try:
        freq = zipf_frequency(word, 'de')
        if freq == 0: return "‚Äî"
        if freq > 5.5: return "A1"
        if freq > 4.5: return "A2"
        if freq > 3.8: return "B1"
        if freq > 2.8: return "B2"
        return "C1"
    except: return "?"

@st.cache_data
def get_translation(word):
    try: return GoogleTranslator(source='de', target='ru').translate(word)
    except: return "-"

@st.cache_data
def get_synonyms(word):
    """
    –£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏.
    """
    # –ó–∞–≥–æ–ª–æ–≤–∫–∏, —á—Ç–æ–±—ã –ø—Ä–∏—Ç–≤–æ—Ä–∏—Ç—å—Å—è –±—Ä–∞—É–∑–µ—Ä–æ–º (–≤–∞–∂–Ω–æ!)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    def fetch_api(query):
        url = f"https://www.openthesaurus.de/synonyme/search?q={query}&format=json"
        try:
            # –£–≤–µ–ª–∏—á–∏–ª–∏ —Ç–∞–π–º-–∞—É—Ç –¥–æ 5 —Å–µ–∫—É–Ω–¥
            r = requests.get(url, headers=headers, timeout=5)
            if r.status_code == 200:
                data = r.json()
                found = []
                for synset in data.get('synsets', []):
                    for term in synset.get('terms', []):
                        t = re.sub(r"\(.*?\)", "", term.get('term')).strip()
                        # –§–∏–ª—å—Ç—Ä: –Ω–µ —Å–∞–º–æ —Å–ª–æ–≤–æ, –Ω–µ —Ñ—Ä–∞–∑–∞
                        if t.lower() != query.lower() and len(t.split()) < 3:
                            found.append(t)
                return list(dict.fromkeys(found))
        except: return []
        return []

    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã API –Ω–µ –∑–∞–±–∞–Ω–∏–ª –Ω–∞—Å –∑–∞ —Å–ø–∞–º
    time.sleep(0.1) 
    
    syns = fetch_api(word)
    
    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º —É–±—Ä–∞—Ç—å –æ–∫–æ–Ω—á–∞–Ω–∏—è (Plural -> Singular)
    if not syns and len(word) > 4:
        if word.endswith("en"): syns = fetch_api(word[:-2])
        elif word.endswith("s") or word.endswith("n") or word.endswith("e"): 
            syns = fetch_api(word[:-1])
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-4
    return ", ".join(syns[:4]) if syns else "‚Äî"

def process_text_chunk(text):
    # –û—Å—Ç–∞–≤–ª—è–µ–º —É–º–ª–∞—É—Ç—ã –∏ –±—É–∫–≤—ã
    clean_text = re.sub(r'[^a-zA-Z√§√∂√º√Ñ√ñ√ú√ü\s]', '', text)
    words = clean_text.split()
    filtered = []
    for w in words:
        # –§–∏–ª—å—Ç—Ä: –¥–ª–∏–Ω–∞ > 3, –Ω–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ, –Ω–µ —á–∏—Å–ª–æ, –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ë–æ–ª—å—à–æ–π (—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ)
        # –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –≤–∞–∂–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã.
        if len(w) >= 4 and w.lower() not in STOP_WORDS and not w.isdigit():
            filtered.append(w)
    return filtered

def find_context(text, word):
    # –ò—â–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    sentences = re.split(r'(?<=[.!?])\s+', text)
    for sent in sentences:
        if re.search(r'\b' + re.escape(word) + r'\b', sent, re.IGNORECASE):
            clean = sent.replace("\n", " ").strip()
            return clean[:150]
    return "‚Äî"

def process_pdf_full(file_obj, start_p, num_pages):
    all_words = []
    full_context_text = ""
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    file_bytes = file_obj.read()
    
    for i in range(num_pages):
        current_page_idx = start_p - 1 + i
        status_text.text(f"‚è≥ –°–∫–∞–Ω–∏—Ä—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {start_p + i}...")
        
        page_text = ""
        
        # 1. –¢–µ–∫—Å—Ç –∏–∑ PDF
        try:
            with pdfplumber.open(file_obj) as pdf:
                if current_page_idx < len(pdf.pages):
                    page_text = pdf.pages[current_page_idx].extract_text()
        except: pass

        # 2. OCR (–µ—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –µ–≥–æ –º–∞–ª–æ)
        if not page_text or len(page_text) < 100:
            try:
                images = convert_from_bytes(
                    file_bytes, 
                    first_page=current_page_idx+1, 
                    last_page=current_page_idx+1
                )
                if images:
                    # psm 6 = block of text (—Ö–æ—Ä–æ—à–æ –¥–ª—è –∫–Ω–∏–≥)
                    config = r'--psm 6' 
                    page_text = pytesseract.image_to_string(images[0], lang='deu', config=config)
                    del images
                    gc.collect()
            except Exception: pass

        if page_text:
            words_in_page = process_text_chunk(page_text)
            all_words.extend(words_in_page)
            full_context_text += page_text + "\n"
        
        progress_bar.progress((i + 1) / num_pages)

    return all_words, full_context_text

# --- 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

st.title("üá©üá™ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å–ª–æ–≤ –¥–ª—è Quizlet (B2)")
st.markdown("–ó–∞–≥—Ä—É–∑–∏ —É—á–µ–±–Ω–∏–∫, –≤—ã–±–µ—Ä–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –∏ —è —Å–æ–∑–¥–∞–º —Ñ–∞–π–ª —Å **—Å–∏–Ω–æ–Ω–∏–º–∞–º–∏** –∏ **–ø–µ—Ä–µ–≤–æ–¥–æ–º**.")

if 'vocab_df' not in st.session_state:
    st.session_state.vocab_df = pd.DataFrame()

with st.sidebar:
    st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    st.info("–õ—É—á—à–∏–µ —Ç–µ–∫—Å—Ç—ã –≤ —Ç–≤–æ–µ–π –∫–Ω–∏–≥–µ: \n- –°—Ç—Ä. 15 (–ü–∏—Ç–∞–Ω–∏–µ)\n- –°—Ç—Ä. 54 (Resilienz)\n- –°—Ç—Ä. 69 (–ó–∞–≤—Ç—Ä–∞–∫)")
    
    start_page = st.number_input("–ù–∞—á–∞—Ç—å —Å–æ —Å—Ç—Ä.", 1, 500, 54)
    pages_to_scan = st.number_input("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü —á–∏—Ç–∞—Ç—å?", 1, 50, 1)
    max_vocab_size = st.slider("–°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", 10, 100, 20)

uploaded_file = st.file_uploader("PDF —Ñ–∞–π–ª", type=['pdf'])

if uploaded_file and st.button("üöÄ –°–æ–∑–¥–∞—Ç—å —Å–ª–æ–≤–∞—Ä—å"):
    st.session_state.vocab_df = pd.DataFrame()
    
    with st.spinner("–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞..."):
        uploaded_file.seek(0)
        raw_words, full_text = process_pdf_full(uploaded_file, start_page, pages_to_scan)
        
    if not raw_words:
        st.error("–°–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
    else:
        st.info(f"–ù–∞–π–¥–µ–Ω–æ {len(raw_words)} —Å–ª–æ–≤. –ü–µ—Ä–µ–≤–æ–∂—É –∏ –∏—â—É —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —Ç–æ–ø-{max_vocab_size}...")
        
        # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É
        top_words_tuples = Counter(raw_words).most_common(max_vocab_size)
        
        data = []
        vocab_bar = st.progress(0)
        
        for idx, (word, count) in enumerate(top_words_tuples):
            if len(word) < 3: continue
                
            lvl = estimate_level(word)
            trans = get_translation(word)
            syns = get_synonyms(word) # –¢–ï–ü–ï–†–¨ –†–ê–ë–û–¢–ê–ï–¢ –õ–£–ß–®–ï
            ctx = find_context(full_text, word)
            
            data.append({
                "–°–ª–æ–≤–æ": word,
                "–ü–µ—Ä–µ–≤–æ–¥": trans,
                "–°–∏–Ω–æ–Ω–∏–º—ã": syns,
                "–£—Ä–æ–≤–µ–Ω—å": lvl,
                "–ö–æ–Ω—Ç–µ–∫—Å—Ç": ctx
            })
            vocab_bar.progress((idx + 1) / len(top_words_tuples))
            
        st.session_state.vocab_df = pd.DataFrame(data)
        st.success("–ì–æ—Ç–æ–≤–æ! –°–∏–Ω–æ–Ω–∏–º—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

# --- 4. –†–µ–∑—É–ª—å—Ç–∞—Ç –∏ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ ---

if not st.session_state.vocab_df.empty:
    df = st.session_state.vocab_df
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É (–º–æ–∂–Ω–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å)
    edited_df = st.data_editor(df, hide_index=True)
    
    st.write("### üì• –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã")
    c1, c2 = st.columns(2)
    
    with c1:
        # –î–ª—è Excel/GitHub
        csv = edited_df.to_csv(index=False, sep=';').encode('utf-8-sig')
        st.download_button("üíæ CSV –¥–ª—è Excel/GitHub", csv, "wortschatz.csv", "text/csv")
        
    with c2:
        # –î–ª—è Quizlet (–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        # –§–æ—Ä–º–∞—Ç: –°–ª–æ–≤–æ (tab) –ü–µ—Ä–µ–≤–æ–¥; –°–∏–Ω–æ–Ω–∏–º—ã
        quizlet_text = ""
        for index, row in edited_df.iterrows():
            term = row['–°–ª–æ–≤–æ']
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–µ—Ä–µ–≤–æ–¥ –∏ —Å–∏–Ω–æ–Ω–∏–º—ã –≤ "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
            defin = f"{row['–ü–µ—Ä–µ–≤–æ–¥']} (Syn: {row['–°–∏–Ω–æ–Ω–∏–º—ã']}) [Lvl: {row['–£—Ä–æ–≤–µ–Ω—å']}]"
            quizlet_text += f"{term}\t{defin}\n"
            
        st.download_button("ü¶â –§–∞–π–ª –¥–ª—è Quizlet", quizlet_text.encode('utf-8'), "quizlet_import.txt", "text/plain")
        
    st.info("**–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è Quizlet:**\n1. –ù–∞–∂–º–∏ 'Create set'.\n2. –ù–∞–∂–º–∏ —Å—Å—ã–ª–∫—É 'Import from Word, Excel...'.\n3. –°–∫–æ–ø–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏–∑ —Å–∫–∞—á–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏ –≤—Å—Ç–∞–≤—å –≤ –ø–æ–ª–µ.")
