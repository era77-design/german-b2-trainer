import streamlit as st
import re
from collections import Counter
from PIL import Image
import pytesseract
import pdfplumber
from pdf2image import convert_from_bytes
import requests
from deep_translator import GoogleTranslator
from wordfreq import zipf_frequency # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —É—Ä–æ–≤–Ω—è —Å–ª–æ–≤

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
st.set_page_config(page_title="DE Tutor Pro", layout="wide")
st.title("üá©üá™ –ù–µ–º–µ—Ü–∫–∏–π: –°–ª–æ–≤–∞—Ä—å, –£—Ä–æ–≤–Ω–∏ –∏ –ü—Ä–∏–º–µ—Ä—ã")

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä. –ü–æ–ª–µ–∑–Ω—ã–µ –≥–ª–∞–≥–æ–ª—ã –∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Å—Ç–∞–≤–ª—è–µ–º.
STOP_WORDS = {
    "der", "die", "das", "und", "ist", "in", "zu", "den", "dem", "des", 
    "mit", "auf", "f√ºr", "von", "ein", "eine", "einen", "sich", "aus",
    "dass", "nicht", "war", "aber", "man", "bei", "wie", "wir", "oder",
    "kann", "sind", "werden", "wird", "auch", "noch", "nur", "vor", "nach",
    "√ºber", "wenn", "zum", "zur", "habe", "hat", "durch", "unter", "diese",
    "dieser", "ihre", "seine", "meine", "vom", "am", "im", "um", "als",
    "es", "sie", "er", "du", "ich", "mich", "mir", "dir", "uns", "ihnen"
}

# --- 2. –§—É–Ω–∫—Ü–∏–∏ ---

@st.cache_data
def estimate_level(word):
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–≤–∞ (A1-C1) –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —à–∫–∞–ª—É Zipf (–æ—Ç 1 –¥–æ 7).
    """
    freq = zipf_frequency(word, 'de')
    
    if freq == 0: return "N/A" # –°–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ
    if freq > 5.5: return "A1" # –û—á–µ–Ω—å —á–∞—Å—Ç–æ–µ
    if freq > 4.5: return "A2"
    if freq > 4.0: return "B1"
    if freq > 3.0: return "B2"
    return "C1+" # –†–µ–¥–∫–æ–µ

@st.cache_data
def get_translation(word):
    try:
        return GoogleTranslator(source='de', target='ru').translate(word)
    except:
        return "-"

@st.cache_data
def get_synonyms(word):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å–∏–Ω–æ–Ω–∏–º–æ–≤"""
    url = f"https://www.openthesaurus.de/synonyme/search?q={word}&format=json"
    try:
        response = requests.get(url, timeout=3)
        data = response.json()
        synonyms = []
        
        # –ü–µ—Ä–µ–±–∏—Ä–∞–µ–º –≤—Å–µ –≥—Ä—É–ø–ø—ã —Å–∏–Ω–æ–Ω–∏–º–æ–≤
        for synset in data.get('synsets', []):
            for term in synset.get('terms', []):
                term_word = term.get('term')
                # –§–∏–ª—å—Ç—Ä—É–µ–º: –Ω–µ —Å–∞–º–æ —Å–ª–æ–≤–æ, –Ω–µ —Ñ—Ä–∞–∑—ã –∏–∑ 3+ —Å–ª–æ–≤
                if term_word.lower() != word.lower() and len(term_word.split()) < 3:
                    # –£–±–∏—Ä–∞–µ–º —Å–∫–æ–±–∫–∏ —Ç–∏–ø–∞ "(ugs.)"
                    clean_syn = re.sub(r"\(.*?\)", "", term_word).strip()
                    synonyms.append(clean_syn)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –±–µ—Ä–µ–º —Ç–æ–ø-3
        unique = list(dict.fromkeys(synonyms))
        return ", ".join(unique[:3])
    except:
        return ""

def find_context(text, word):
    """–ò—â–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å —ç—Ç–∏–º —Å–ª–æ–≤–æ–º –≤ —Ç–µ–∫—Å—Ç–µ"""
    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–ø–æ —Ç–æ—á–∫–µ, –≤–æ–ø—Ä–æ—Å—É, –≤–æ—Å–∫–ª. –∑–Ω–∞–∫—É)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    for sent in sentences:
        # –ò—â–µ–º —Ü–µ–ª–æ–µ —Å–ª–æ–≤–æ (—á—Ç–æ–±—ã "in" –Ω–µ –Ω–∞—Ö–æ–¥–∏–ª–æ "Berlin")
        if re.search(r'\b' + re.escape(word) + r'\b', sent, re.IGNORECASE):
            clean_sent = sent.replace("\n", " ").strip()
            if len(clean_sent) > 200: return clean_sent[:200] + "..."
            return clean_sent
    return "-"

def extract_text(file_bytes, file_type, start, limit):
    text = ""
    error = None
    start_idx = start - 1
    
    # 1. –ü–æ–ø—ã—Ç–∫–∞ PDF text
    if file_type == "application/pdf":
        try:
            with pdfplumber.open(file_bytes) as pdf:
                pages = pdf.pages[start_idx : start_idx + limit]
                for p in pages:
                    t = p.extract_text()
                    if t: text += t + "\n"
        except: pass

    # 2. –ü–æ–ø—ã—Ç–∫–∞ OCR
    if len(text) < 50:
        if file_type == "application/pdf":
            st.warning(f"üìÑ –†–∞–±–æ—Ç–∞–µ—Ç OCR (—Å—Ç—Ä. {start}-{start+limit-1})...")
            try:
                file_bytes.seek(0)
                images = convert_from_bytes(file_bytes.read(), first_page=start, last_page=start+limit-1)
                bar = st.progress(0)
                for i, img in enumerate(images):
                    text += pytesseract.image_to_string(img, lang='deu') + "\n"
                    bar.progress((i+1)/len(images))
            except Exception as e: error = str(e)
        else:
            img = Image.open(file_bytes)
            text = pytesseract.image_to_string(img, lang='deu')
            
    return text, error

def process_text(text, min_len):
    # –£–±–∏—Ä–∞–µ–º –≤—Å—ë –∫—Ä–æ–º–µ –±—É–∫–≤
    clean_text = re.sub(r'[^a-zA-Z√§√∂√º√Ñ√ñ√ú√ü\s]', '', text)
    words = clean_text.split()
    
    filtered = []
    for w in words:
        w_clean = w.strip()
        if len(w_clean) >= min_len and w_clean.lower() not in STOP_WORDS:
            filtered.append(w_clean)
            
    return Counter(filtered).most_common()

# --- 3. –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---

with st.sidebar:
    st.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")
    start_page = st.number_input("–ù–∞—á–∞—Ç—å —Å–æ —Å—Ç—Ä.", 1, 100, 7)
    pages_to_read = st.slider("–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü", 1, 5, 2)
    min_word_len = st.slider("–ú–∏–Ω. –¥–ª–∏–Ω–∞ —Å–ª–æ–≤–∞", 2, 8, 3, help="–°—Ç–∞–≤—å 3, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —Å–ª–æ–≤–∞ —Ç–∏–ø–∞ 'tun' –∏–ª–∏ 'neu'")
    max_words_show = st.slider("–°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –ø–æ–∫–∞–∑–∞—Ç—å", 10, 50, 20)

st.write("### üá©üá™ –£–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞")
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏ PDF –∏–ª–∏ —Ñ–æ—Ç–æ", type=['pdf', 'jpg', 'png'])

if uploaded_file and st.button("üöÄ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å"):
    with st.spinner("–ß–∏—Ç–∞–µ–º, –ø–µ—Ä–µ–≤–æ–¥–∏–º, –æ—Ü–µ–Ω–∏–≤–∞–µ–º —É—Ä–æ–≤–µ–Ω—å..."):
        full_text, err = extract_text(uploaded_file, uploaded_file.type, start_page, pages_to_read)
        
        if err:
            st.error(f"–û—à–∏–±–∫–∞: {err}")
        elif len(full_text) < 10:
            st.warning("–¢–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.")
        else:
            # 1. –ß–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            freq_list = process_text(full_text, min_word_len)
            top_words = freq_list[:max_words_show]
            
            # 2. –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
            table_data = []
            prog = st.progress(0)
            
            for i, (word, count) in enumerate(top_words):
                lvl = estimate_level(word)
                trans = get_translation(word)
                syns = get_synonyms(word)
                ctx = find_context(full_text, word)
                
                table_data.append({
                    "–£—Ä–æ–≤–µ–Ω—å": lvl,
                    "–°–ª–æ–≤–æ": word,
                    "–ü–µ—Ä–µ–≤–æ–¥": trans,
                    "–°–∏–Ω–æ–Ω–∏–º—ã": syns if syns else "‚Äî",
                    "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞": ctx,
                    "–í —Å–ª–æ–≤–∞—Ä—å": False
                })
                prog.progress((i+1)/len(top_words))
            
            st.success("–ì–æ—Ç–æ–≤–æ!")
            
            # 3. –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã
            st.data_editor(
                table_data,
                column_config={
                    "–£—Ä–æ–≤–µ–Ω—å": st.column_config.TextColumn("Uvl", help="A1-C2 (–æ—Ü–µ–Ω–∫–∞)", width="small"),
                    "–í —Å–ª–æ–≤–∞—Ä—å": st.column_config.CheckboxColumn("‚úÖ"),
                    "–ü—Ä–∏–º–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞": st.column_config.TextColumn("–ö–æ–Ω—Ç–µ–∫—Å—Ç", width="large")
                },
                height=800,
                hide_index=True
            )
